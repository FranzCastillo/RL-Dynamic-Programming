{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Programación Dinámica\n",
    "Francisco Castillo - 21562\n",
    "Diego Lemus -"
   ],
   "id": "dc576fcd0dade002"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 1\n",
    "### ¿Qué es la Programación Dinámica y cómo se relaciona con Reinforcement Learning?\n",
    "Es un paradigma de programación que se utiliza para resolver problemas que pueden descomponerse en otros subproblemas más simples. En los Markov Decision Processes, la programación dinámica permite encontrar la política óptima y el valor de los estados mediante la iteración de políticas o la iteración de valores mientras se conozcan las dinámicas del entorno (probabilidades de transición y recompensas).\n",
    "### Explique en sus propias palabras el algoritmo de Iteración de Póliza\n",
    "El algoritmo de Policy Iteration es un método para encontrar la política óptima en un MDP. Consiste en dos pasos:\n",
    "1. **Evaluación de la política:** se calcula el valor de los estados bajo la política actual.\n",
    "2. **Mejora de la política:** se actualiza la política seleccionando acciones que maximicen el valor esperado de los estados. Este proceso se repite hasta que la política converge.\n",
    "### Explique en sus propias palabras el algoritmo de Iteración de Valor\n",
    "El algoritmo de Value Iteration es un método para encontrar la política óptima en un MDP mediante la actualización iterativa de los valores de los estados. Este solo tiene un paso:\n",
    "1. **Actualización de valores:** se actualizan los valores de los estados utilizando la ecuación de Bellman, considerando las acciones posibles y las probabilidades de transición. Este proceso se repite hasta que los valores convergen, lo que permite derivar la política óptima a partir de los valores finales.\n",
    "### En el laboratorio pasado, vimos que el valor de los premios obtenidos se mantienen constantes, ¿por qué?\n",
    "Se mantienen constantes porque era un ambiente determinista con recomepnsas fijas. Esto implica que la función de recomensa no cambia con el tiempo y las recompensas son predecibles.\n",
    "\n",
    "> [Value Iteration vs. Policy Iteration](https://www.geeksforgeeks.org/data-science/what-is-the-difference-between-value-iteration-and-policy-iteration/)"
   ],
   "id": "c7d467cb7a36e433"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Task 2",
   "id": "1aaf033da5d7e9e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T00:36:49.520292Z",
     "start_time": "2025-07-28T00:36:49.460037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from obj.policy import Policy\n",
    "from obj.maze import Maze\n",
    "from obj.simulator import Simulator"
   ],
   "id": "4ae90d650d8293fb",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T00:36:49.574564Z",
     "start_time": "2025-07-28T00:36:49.546200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "maze = Maze()\n",
    "policy = Policy()\n",
    "simulator = Simulator(maze, policy)\n",
    "\n",
    "maze.print_transition_matrix()\n",
    "maze.print_reward_function()\n",
    "policy.print_policy()\n",
    "\n",
    "simulator.simulate_policy_trace()\n",
    "\n",
    "avg = simulator.evaluate_policy(runs=100)\n",
    "print(f\"Recompensa promedio tras 100 simulaciones: {avg:.2f}\")"
   ],
   "id": "dba4221177d54b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de transición P[s][a]:\n",
      "\n",
      "Estado 0:\n",
      "  'UP': {0: 1.0}\n",
      "\n",
      "  'DOWN': {3: 1.0}\n",
      "\n",
      "  'LEFT': {0: 1.0}\n",
      "\n",
      "  'RIGHT': {1: 1.0}\n",
      "\n",
      "Estado 1:\n",
      "  'UP': {1: 1.0}\n",
      "\n",
      "  'DOWN': {1: 1.0}\n",
      "\n",
      "  'LEFT': {0: 1.0}\n",
      "\n",
      "  'RIGHT': {1: 1.0}\n",
      "\n",
      "Estado 2:\n",
      "  'UP': {2: 1.0}\n",
      "\n",
      "  'DOWN': {5: 1.0}\n",
      "\n",
      "  'LEFT': {1: 1.0}\n",
      "\n",
      "  'RIGHT': {2: 1.0}\n",
      "\n",
      "Estado 3:\n",
      "  'UP': {0: 1.0}\n",
      "\n",
      "  'DOWN': {6: 1.0}\n",
      "\n",
      "  'LEFT': {3: 1.0}\n",
      "\n",
      "  'RIGHT': {3: 1.0}\n",
      "\n",
      "Estado 4:\n",
      "  'UP': {1: 1.0}\n",
      "\n",
      "  'DOWN': {7: 1.0}\n",
      "\n",
      "  'LEFT': {3: 1.0}\n",
      "\n",
      "  'RIGHT': {5: 1.0}\n",
      "\n",
      "Estado 5:\n",
      "  'UP': {5: 1.0}\n",
      "\n",
      "  'DOWN': {8: 1.0}\n",
      "\n",
      "  'LEFT': {5: 1.0}\n",
      "\n",
      "  'RIGHT': {5: 1.0}\n",
      "\n",
      "Estado 6:\n",
      "  'UP': {3: 1.0}\n",
      "\n",
      "  'DOWN': {6: 1.0}\n",
      "\n",
      "  'LEFT': {6: 1.0}\n",
      "\n",
      "  'RIGHT': {7: 1.0}\n",
      "\n",
      "Estado 7:\n",
      "  'UP': {7: 1.0}\n",
      "\n",
      "  'DOWN': {7: 1.0}\n",
      "\n",
      "  'LEFT': {6: 1.0}\n",
      "\n",
      "  'RIGHT': {8: 1.0}\n",
      "\n",
      "Estado 8:\n",
      "  'UP': {5: 1.0}\n",
      "\n",
      "  'DOWN': {8: 1.0}\n",
      "\n",
      "  'LEFT': {7: 1.0}\n",
      "\n",
      "  'RIGHT': {8: 1.0}\n",
      "\n",
      "Función de recompensa R[s][a][s']:\n",
      "\n",
      "Estado 0:\n",
      "  'UP' → 0: -25\n",
      "\n",
      "  'DOWN' → 3: 0\n",
      "\n",
      "  'LEFT' → 0: -25\n",
      "\n",
      "  'RIGHT' → 1: 0\n",
      "\n",
      "Estado 1:\n",
      "  'UP' → 1: -25\n",
      "\n",
      "  'DOWN' → 1: -25\n",
      "\n",
      "  'LEFT' → 0: 0\n",
      "\n",
      "  'RIGHT' → 1: -25\n",
      "\n",
      "Estado 2:\n",
      "  'UP' → 2: -25\n",
      "\n",
      "  'DOWN' → 5: 0\n",
      "\n",
      "  'LEFT' → 1: 0\n",
      "\n",
      "  'RIGHT' → 2: -25\n",
      "\n",
      "Estado 3:\n",
      "  'UP' → 0: 0\n",
      "\n",
      "  'DOWN' → 6: 0\n",
      "\n",
      "  'LEFT' → 3: -25\n",
      "\n",
      "  'RIGHT' → 3: -25\n",
      "\n",
      "Estado 4:\n",
      "  'UP' → 1: 0\n",
      "\n",
      "  'DOWN' → 7: 0\n",
      "\n",
      "  'LEFT' → 3: 0\n",
      "\n",
      "  'RIGHT' → 5: 0\n",
      "\n",
      "Estado 5:\n",
      "  'UP' → 5: -25\n",
      "\n",
      "  'DOWN' → 8: 100\n",
      "\n",
      "  'LEFT' → 5: -25\n",
      "\n",
      "  'RIGHT' → 5: -25\n",
      "\n",
      "Estado 6:\n",
      "  'UP' → 3: 0\n",
      "\n",
      "  'DOWN' → 6: -25\n",
      "\n",
      "  'LEFT' → 6: -25\n",
      "\n",
      "  'RIGHT' → 7: 0\n",
      "\n",
      "Estado 7:\n",
      "  'UP' → 7: -25\n",
      "\n",
      "  'DOWN' → 7: -25\n",
      "\n",
      "  'LEFT' → 6: 0\n",
      "\n",
      "  'RIGHT' → 8: 100\n",
      "\n",
      "Estado 8:\n",
      "  'UP' → 5: 0\n",
      "\n",
      "  'DOWN' → 8: 100\n",
      "\n",
      "  'LEFT' → 7: 0\n",
      "\n",
      "  'RIGHT' → 8: 100\n",
      "\n",
      "Política π (estado → acción):\n",
      "\n",
      "  Estado 0: 'DOWN'\n",
      "\n",
      "  Estado 1: 'LEFT'\n",
      "\n",
      "  Estado 2: 'LEFT'\n",
      "\n",
      "  Estado 3: 'DOWN'\n",
      "\n",
      "  Estado 4: 'LEFT'\n",
      "\n",
      "  Estado 5: 'DOWN'\n",
      "\n",
      "  Estado 6: 'RIGHT'\n",
      "\n",
      "  Estado 7: 'RIGHT'\n",
      "\n",
      "  Estado 8: 'UP'\n",
      "\n",
      "Simulación paso a paso:\n",
      " Paso 1: 0 --[DOWN]--> 3 | Recompensa = 0\n",
      " Paso 2: 3 --[DOWN]--> 6 | Recompensa = 0\n",
      " Paso 3: 6 --[RIGHT]--> 7 | Recompensa = 0\n",
      " Paso 4: 7 --[RIGHT]--> 8 | Recompensa = 100\n",
      "Recompensa acumulada total: 100\n",
      "\n",
      "Recompensa promedio tras 100 simulaciones: 100.00\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f40ed5bcee1b260f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
