{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc576fcd0dade002",
   "metadata": {},
   "source": [
    "# Programación Dinámica\n",
    "- Francisco Castillo - 21562\n",
    "- Diego Lemus - 21469"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c390ea3e",
   "metadata": {},
   "source": [
    "## Task 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d467cb7a36e433",
   "metadata": {},
   "source": [
    "### ¿Qué es la Programación Dinámica y cómo se relaciona con Reinforcement Learning?\n",
    "Es un paradigma de programación que se utiliza para resolver problemas que pueden descomponerse en otros subproblemas más simples. En los Markov Decision Processes, la programación dinámica permite encontrar la política óptima y el valor de los estados mediante la iteración de políticas o la iteración de valores mientras se conozcan las dinámicas del entorno (probabilidades de transición y recompensas).\n",
    "### Explique en sus propias palabras el algoritmo de Iteración de Póliza\n",
    "El algoritmo de Policy Iteration es un método para encontrar la política óptima en un MDP. Consiste en dos pasos:\n",
    "1. **Evaluación de la política:** se calcula el valor de los estados bajo la política actual.\n",
    "2. **Mejora de la política:** se actualiza la política seleccionando acciones que maximicen el valor esperado de los estados. Este proceso se repite hasta que la política converge.\n",
    "### Explique en sus propias palabras el algoritmo de Iteración de Valor\n",
    "El algoritmo de Value Iteration es un método para encontrar la política óptima en un MDP mediante la actualización iterativa de los valores de los estados. Este solo tiene un paso:\n",
    "1. **Actualización de valores:** se actualizan los valores de los estados utilizando la ecuación de Bellman, considerando las acciones posibles y las probabilidades de transición. Este proceso se repite hasta que los valores convergen, lo que permite derivar la política óptima a partir de los valores finales.\n",
    "### En el laboratorio pasado, vimos que el valor de los premios obtenidos se mantienen constantes, ¿por qué?\n",
    "Se mantienen constantes porque era un ambiente determinista con recomepnsas fijas. Esto implica que la función de recomensa no cambia con el tiempo y las recompensas son predecibles.\n",
    "\n",
    "> [Value Iteration vs. Policy Iteration](https://www.geeksforgeeks.org/data-science/what-is-the-difference-between-value-iteration-and-policy-iteration/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaf033da5d7e9e7",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ae90d650d8293fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T00:36:49.520292Z",
     "start_time": "2025-07-28T00:36:49.460037Z"
    }
   },
   "outputs": [],
   "source": [
    "from obj.policy import Policy\n",
    "from obj.maze import Maze\n",
    "from obj.simulator import Simulator\n",
    "from obj.mdp_solver import MDP_Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dba4221177d54b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T00:36:49.574564Z",
     "start_time": "2025-07-28T00:36:49.546200Z"
    }
   },
   "outputs": [],
   "source": [
    "maze = Maze()\n",
    "policy = Policy()\n",
    "simulator = Simulator(maze, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f40ed5bcee1b260f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Política dada ===\n",
      "Política π (estado → acción):\n",
      "\n",
      "  Estado 0: 'DOWN'\n",
      "\n",
      "  Estado 1: 'LEFT'\n",
      "\n",
      "  Estado 2: 'LEFT'\n",
      "\n",
      "  Estado 3: 'DOWN'\n",
      "\n",
      "  Estado 4: 'LEFT'\n",
      "\n",
      "  Estado 5: 'DOWN'\n",
      "\n",
      "  Estado 6: 'RIGHT'\n",
      "\n",
      "  Estado 7: 'RIGHT'\n",
      "\n",
      "  Estado 8: 'UP'\n",
      "\n",
      "Simulación paso a paso:\n",
      " Paso 1: 0 --[DOWN]--> 3 | Recompensa = 0\n",
      " Paso 2: 3 --[DOWN]--> 6 | Recompensa = 0\n",
      " Paso 3: 6 --[RIGHT]--> 7 | Recompensa = 0\n",
      " Paso 4: 7 --[RIGHT]--> 8 | Recompensa = 100\n",
      "Recompensa acumulada total: 100\n",
      "\n",
      "Recompensa promedio tras 100 simulaciones (política dada): 100.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Política dada ===\")\n",
    "policy.print_policy()\n",
    "simulator.simulate_policy_trace()\n",
    "avg = simulator.evaluate_policy(runs=100)\n",
    "print(f\"Recompensa promedio tras 100 simulaciones (política dada): {avg:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dd2ea8",
   "metadata": {},
   "source": [
    "### Iteración de Valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Resultados Iteración de Valor ===\n",
      "Función de valor óptima V*:\n",
      "  V(0) = 72.900\n",
      "  V(1) = 65.610\n",
      "  V(2) = 90.000\n",
      "  V(3) = 81.000\n",
      "  V(4) = 90.000\n",
      "  V(5) = 100.000\n",
      "  V(6) = 90.000\n",
      "  V(7) = 100.000\n",
      "  V(8) = 0.000\n",
      "\n",
      "Política óptima extraída π* (de V*):\n",
      "  Estado 0: DOWN\n",
      "  Estado 1: LEFT\n",
      "  Estado 2: DOWN\n",
      "  Estado 3: DOWN\n",
      "  Estado 4: DOWN\n",
      "  Estado 5: DOWN\n",
      "  Estado 6: RIGHT\n",
      "  Estado 7: RIGHT\n",
      "  Estado 8: -\n"
     ]
    }
   ],
   "source": [
    "solver_vi = MDP_Solver(maze, gamma=0.9, theta=0.001)\n",
    "V_opt = solver_vi.value_iteration()\n",
    "pi_from_vi = solver_vi.extract_policy()\n",
    "\n",
    "print(\"=== Resultados Iteración de Valor ===\")\n",
    "print(\"Función de valor óptima V*:\")\n",
    "for s in sorted(V_opt):\n",
    "    print(f\"  V({s}) = {V_opt[s]:.3f}\")\n",
    "print(\"\\nPolítica óptima extraída π* (de V*):\")\n",
    "for s in sorted(pi_from_vi):\n",
    "    action = pi_from_vi[s] or \"-\"\n",
    "    print(f\"  Estado {s}: {action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b81d00",
   "metadata": {},
   "source": [
    "#### Análisis\n",
    "\n",
    "- Estados cercanos a la meta (5,7) alcanzan el valor máximo descontado de 100.\n",
    "- El estado terminal 8 tiene V*(8)=0 porque allí termina el proceso.\n",
    "- Un agente que empiece en el estado 0 puede esperar en promedio unas 72.9 unidades de recompensa descontadas antes de llegar a la meta si sigue la política óptima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cca9356",
   "metadata": {},
   "source": [
    "### Iteración de Política"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dac5eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Resultados Iteración de Política ===\n",
      "Política óptima π* (policy iteration):\n",
      "  Estado 0: DOWN\n",
      "  Estado 1: LEFT\n",
      "  Estado 2: DOWN\n",
      "  Estado 3: DOWN\n",
      "  Estado 4: DOWN\n",
      "  Estado 5: DOWN\n",
      "  Estado 6: RIGHT\n",
      "  Estado 7: RIGHT\n",
      "  Estado 8: RIGHT\n",
      "\n",
      "Función de valor resultante V(s) para π*:\n",
      "  V(0) = 72.900\n",
      "  V(1) = 65.610\n",
      "  V(2) = 90.000\n",
      "  V(3) = 81.000\n",
      "  V(4) = 90.000\n",
      "  V(5) = 100.000\n",
      "  V(6) = 90.000\n",
      "  V(7) = 100.000\n",
      "  V(8) = 0.000\n"
     ]
    }
   ],
   "source": [
    "solver_pi = MDP_Solver(maze, gamma=0.9, theta=0.001)\n",
    "pi_opt = solver_pi.policy_iteration()\n",
    "V_from_pi = solver_pi.V\n",
    "\n",
    "print(\"=== Resultados Iteración de Política ===\")\n",
    "print(\"Política óptima π* (policy iteration):\")\n",
    "for s in sorted(pi_opt):\n",
    "    action = pi_opt[s] or \"-\"\n",
    "    print(f\"  Estado {s}: {action}\")\n",
    "print(\"\\nFunción de valor resultante V(s) para π*:\")\n",
    "for s in sorted(V_from_pi):\n",
    "    print(f\"  V({s}) = {V_from_pi[s]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6aa290",
   "metadata": {},
   "source": [
    "#### Análisis\n",
    "\n",
    "- Policy Iteration en general converge en menos iteraciones que Value Iteration.\n",
    "- La política final coincide prácticamente con la extraída por Value Iteration.\n",
    "- Se confirma que π* es la misma política óptima hallada por ambos métodos para el MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b6f037",
   "metadata": {},
   "source": [
    "### Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf4a0e3",
   "metadata": {},
   "source": [
    "Modelado del MDP: las matrices PP y RR están bien definidas, por ende las transiciones deterministas y recompensas son correctas.\n",
    "\n",
    "\n",
    "Política óptima única: los dos métodos convergen al mismo resultado, lo que demuestra que la política encontrada es, de hecho, la óptima para este problema.\n",
    "\n",
    "\n",
    "Eficacia de la simulación: al usar π* en el simulador, se obtendrá siempre la máxima recompensa esperada, sin riesgo de choque."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
